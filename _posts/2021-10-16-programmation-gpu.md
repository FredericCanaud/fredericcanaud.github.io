---
date: 2021-10-16
layout: post
title: Programmation GPU
subtitle: Pr√©sentation de la programmation GPU avec Nvidia CUDA
description: Pr√©sentation de la programmation GPU avec Nvidia CUDA
image: /assets/img/uploads/programmation-nvidia.webp
optimized_image: /assets/img/uploads/programmation-nvidia.webp
alt: Programmation GPU
category: GPU
type: Experience
tags:
  - GPU
  - CUDA
  - C++
author: fredericcanaud
---

(Les illustrations utilis√©es proviennent du cours de Pierre-Fran√ßois Bonnefoi, source : <a href="https://p-fb.net/master1/gpgpu/cours/cuda.pdf">https://p-fb.net/master1/gpgpu/cours/cuda.pdf</a>)

## √Ä propos de la programmation GPU

La programmation **GPU** (ou GPUGPU pour General-Purpose computing on **Graphics Processing Units**) vise √† exploiter la puissance des calculs qu'est capable une carte graphique, pour des t√¢ches massivement parall√©lisables.

Tandis qu'un CPU est surtout d√©di√© aux traitements de t√¢ches rapides et s√©quentielles, les cartes graphiques apportent une nouvelle architecture de programmation tr√®s favorable au parall√©lisme.
Le pr√©curseur a surtout √©t√© Nvidia qui d√©veloppe depuis 2007 une interface mat√©rielle et un langage de programmation bas√© en C : **CUDA** (**C**ompute **U**nified **D**evice **A**rchitecture)

## Cuda ? Comment √ßa marche ? ü§î

Avec CUDA, on s√©pare l'architecture CPU (appel√© **Host**) de l'architecture GPU (appel√© **Device**).
L'h√¥te poss√®de sa propre RAM contenant ses programmes et ses variables, tandis que le GPU ne s'occupe de n'ex√©cuter que la partie du code qui lui est d√©di√©e, que l'on appelle **kernel**. Les deux architectures s'√©changent des donn√©es via un bus PCI.

![√âchange CPU / GPU - PFB](/assets/img/uploads/echangeGPUCPU.webp)

C'est donc toujours le CPU qui aura le contr√¥le des deux architectures :
- Il alloue de la m√©moire sur la carte graphique gr√¢ce √† l'instruction **cudaMalloc**
- Le CPU copie les donn√©es √† transf√©rer vers la RAM de la carte graphique avec l'instruction **cudaMemcpy**
- Lorsqu'un kernel est appel√©, le CPU transmet l'adresse m√©moire que le GPU doit exploiter lors de l'ex√©cution de son kernel
- Le CPU r√©cup√®re alors le r√©sultat obtenu dans le kernel, avant de d√©sallouer la m√©moire associ√©e avec l'instruction **cudaFree**

## Et √† quoi √ßa ressemble, un GPU ?

Question pas tr√®s √©vidente, cher ami !

![ArchitectureGPU](/assets/img/uploads/gpuArchitecture.webp)

En programmation GPU :

- Ta carte graphique est divis√©e en plusieurs **multiprocesseurs**, qui contiennent donc chacun des **processeurs**
- Tous ces multiprocesseurs partagent une m√™me **m√©moire globale**, en GDDR
- Chaque multiprocesseur partagent une **m√©moire partag√©e**, sur lesquels les processeurs d'un multiprocesseur peuvent √©changer plus rapidement que s'ils passaient par la m√©moire globale, mais attention aux **acc√®s concurrents** !
- Chaque processeur poss√®de ses propres **registres**, mais de quantit√© plus petite, de l'ordre de 16 kilooctets

En CUDA, la m√©moire est construire similairement, mais de mani√®re hi√©rarchique :

- Au niveau le plus bas, on trouve la **thread**, qui poss√®de sa propre m√©moire locale
- Vient ensuite les **blocs**, compos√© de plusieurs threads ex√©cut√©es de mani√®re **concurrente**, et ayant acc√®s √† une m√©moire partag√©e
- Au niveau le plus haut, on trouve les **grilles**, compos√©s de plusieurs blocs de threads, qui utilisent la m√©moire globale

Il existe √©galement la notion de **warp** au sein d'un bloc (un wrap √©tant un groupe de 32 threads synchronis√©es), mais si vous souhaitez en savoir plus, je vous invite √† suivre le cours de Pierre-Fran√ßois Bonnefoi üòâ : <a href="https://p-fb.net/master1/gpgpu/cours/cuda.pdf">https://p-fb.net/master1/gpgpu/cours/cuda.pdf</a>

## Les usages de la programmation GPU

√âvidemment, l'usage le plus connu de la programmation GPU reste dans le milieu du **jeu-vid√©o** et des **effets sp√©ciaux**.

Mais on en trouve aussi dans le milieu scientifique : En fournissant une plus forte puissance de calcul qu'avec un CPU, on peut
effectuer des simulations, de l'analyse et du traitement de donn√©es en des temps plus raisonnables !

Durant le Master CRYPTIS (et donc en rapport avec la cryptographie RSA), j'ai pu cr√©er des programmes CUDA permettant de casser la factorisation en nombre premiers de nombres immenses. üòâ